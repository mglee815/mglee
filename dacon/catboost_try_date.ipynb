{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import platform\n",
    "import random\n",
    "import math\n",
    "from typing import List, Dict, Tuple\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from catboost import Pool, CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- os: Linux-5.4.0-91-generic-x86_64-with-glibc2.10\n",
      "- python: 3.8.5 (default, Sep  4 2020, 07:30:14) \n",
      "[GCC 7.3.0]\n",
      "- pandas: 1.1.3\n",
      "- numpy: 1.19.2\n",
      "- sklearn: 0.23.2\n"
     ]
    }
   ],
   "source": [
    "print(f\"- os: {platform.platform()}\")\n",
    "print(f\"- python: {sys.version}\")\n",
    "print(f\"- pandas: {pd.__version__}\")\n",
    "print(f\"- numpy: {np.__version__}\")\n",
    "print(f\"- sklearn: {sklearn.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(501951, 35) (46404, 34)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"/home/mglee/VSCODE/Dacon/Jobcare_data/train.csv\")\n",
    "test_df = pd.read_csv(\"/home/mglee/VSCODE/Dacon/Jobcare_data/test.csv\")\n",
    "\n",
    "code_d = pd.read_csv(\"/home/mglee/VSCODE/Dacon/Jobcare_data/속성_D_코드.csv\")\n",
    "code_h = pd.read_csv(\"/home/mglee/VSCODE/Dacon/Jobcare_data/속성_H_코드.csv\").iloc[:,:-1]\n",
    "code_l = pd.read_csv(\"/home/mglee/VSCODE/Dacon/Jobcare_data/속성_L_코드.csv\")\n",
    "\n",
    "print(train_df.shape, test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_d.columns= [\"attribute_d\",\"attribute_d_d\",\"attribute_d_s\",\"attribute_d_m\",\"attribute_d_l\"]\n",
    "code_h.columns= [\"attribute_h\", \"attribute_h_p\"]\n",
    "code_l.columns= [\"attribute_l\",\"attribute_l_d\",\"attribute_l_s\",\"attribute_l_m\",\"attribute_l_l\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Merge_codes(df:pd.DataFrame, df_code:pd.DataFrame, col:str) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df_code = df_code.copy()\n",
    "    df_code = df_code.add_prefix(f\"{col}_\")\n",
    "    df_code.columns.values[0] = col\n",
    "    return pd.merge(df, df_code, how = 'left', on = col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature_engineering과 Feature_enginerring2가 핵심적인 변화\n",
    "\n",
    "1. Feature_engineering 함수를 활성화 할 경우\n",
    "contents_rn이라는 변수를 제외하는 기존 모델과 다르게 변수로 사용함\n",
    "다만 그대로 사용하지 않고\n",
    "contents_rn을 몇번씩 등장했는지 카운팅해서 그 값을 변수로 사용\n",
    "\n",
    "2. Feature_engineering2 함수를 활성화 할 경우\n",
    "contents_rn과 같은 방식으로 person_rn변수를 전처리함\n",
    "기존 모델은 person_rn 변수를 통으로 사용했음\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#content_rn이라는 변수도 사용하되, 그대로 사용하는 것이 아니라 출현 빈도로 값을 재할당\n",
    "\n",
    "def Feature_engineering(df):\n",
    "    content_freq = df.groupby('contents_rn').count()['id']\n",
    "    df = pd.merge(df, content_freq, how = 'left', on = 'contents_rn')\n",
    "    # df.id_y = np.where(\n",
    "    #     df['id_y'] == 1, 1,\n",
    "    #     np.where(df['id_y'] < 5, 5,\n",
    "    #     np.where(df['id_y'] < 10, 10, 'over10')))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## person_rn도 count해서 사용 (overfitting 방지)\n",
    "\n",
    "def Feature_engineering2(df):\n",
    "    content_freq = df.groupby('contents_rn').count()['id']\n",
    "    df_temp = pd.merge(df, content_freq, how = 'left', on = 'contents_rn')\n",
    "\n",
    "    user_freq = df.groupby('person_rn').count()['id']\n",
    "    df = pd.merge(df_temp, user_freq, how = 'left', on = 'person_rn')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Date_pre(data):\n",
    "    data['contents_open_dt'] = data['contents_open_dt'].astype('str')\n",
    "    DATE = data['contents_open_dt'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d %H:%M:%S'))\n",
    "    \n",
    "    DATE = pd.DataFrame(DATE)\n",
    "    DATE = DATE.rename(columns = {'contents_open_dt': 'date'})\n",
    "    \n",
    "    DATE['Y'] = DATE['date'].apply(lambda x: x.timetuple()[0])\n",
    "    DATE['M'] = DATE['date'].apply(lambda x: x.timetuple()[1])\n",
    "    DATE['D'] = DATE['date'].apply(lambda x: x.timetuple()[2])\n",
    "    DATE['id'] = data['id']\n",
    "    \n",
    "    data = data.merge(DATE, on = 'id', how = 'left')\n",
    "    data = data.drop(columns = ['date', 'contents_open_dt'])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Preprocess_data(\n",
    "    df:pd.DataFrame, is_train:bool = True, cols_merge:List[Tuple[str, pd.DataFrame]] = [], cols_equi:List[Tuple[str, str]] = [],\n",
    "    cols_drop:List[str] = ['id_x', 'person_prefer_f', 'person_perfer_g']) -> Tuple[pd.DataFrame, np.ndarray]:\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    #기존과의 차이점\n",
    "\n",
    "    df = Date_pre(df)\n",
    "\n",
    "    ########################################\n",
    "    #####어떤 방식으로 모델을 학습시킬지는 여기서 결정\n",
    "    df = Feature_engineering2(df)#########\n",
    "    ######################################\n",
    "    y_data = None\n",
    "    if is_train:\n",
    "        y_data = df['target'].to_numpy()\n",
    "        df = df.drop(columns='target')\n",
    "\n",
    "    for col, df_code in cols_merge:\n",
    "        df = Merge_codes(df, df_code, col)\n",
    "    \n",
    "    cols = df.select_dtypes(bool).columns.tolist()\n",
    "    df[cols] = df[cols].astype(int)\n",
    "\n",
    "    for col1, col2 in cols_equi:\n",
    "        df[f\"{col1}_{col2}\"] = (df[col1] == df[col2]).astype(int)\n",
    "    df = df.drop(columns= cols_drop)\n",
    "    \n",
    "    \n",
    "    return (df, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_merge = [\n",
    "              (\"person_prefer_d_1\" , code_d),\n",
    "              (\"person_prefer_d_2\" , code_d),\n",
    "              (\"person_prefer_d_3\" , code_d),\n",
    "              (\"contents_attribute_d\" , code_d),\n",
    "              (\"person_prefer_h_1\" , code_h),\n",
    "              (\"person_prefer_h_2\" , code_h),\n",
    "              (\"person_prefer_h_3\" , code_h),\n",
    "              (\"contents_attribute_h\" , code_h),\n",
    "              (\"contents_attribute_l\" , code_l),\n",
    "]\n",
    "\n",
    "# 회원 속성과 콘텐츠 속성의 동일한 코드 여부에 대한 컬럼명 리스트\n",
    "cols_equi = [\n",
    "\n",
    "    (\"contents_attribute_c\",\"person_prefer_c\"),\n",
    "    (\"contents_attribute_e\",\"person_prefer_e\"),\n",
    "\n",
    "    (\"person_prefer_d_2_attribute_d_s\" , \"contents_attribute_d_attribute_d_s\"),\n",
    "    (\"person_prefer_d_2_attribute_d_m\" , \"contents_attribute_d_attribute_d_m\"),\n",
    "    (\"person_prefer_d_2_attribute_d_l\" , \"contents_attribute_d_attribute_d_l\"),\n",
    "    (\"person_prefer_d_3_attribute_d_s\" , \"contents_attribute_d_attribute_d_s\"),\n",
    "    (\"person_prefer_d_3_attribute_d_m\" , \"contents_attribute_d_attribute_d_m\"),\n",
    "    (\"person_prefer_d_3_attribute_d_l\" , \"contents_attribute_d_attribute_d_l\"),\n",
    "\n",
    "    (\"person_prefer_h_1_attribute_h_p\" , \"contents_attribute_h_attribute_h_p\"),\n",
    "    (\"person_prefer_h_2_attribute_h_p\" , \"contents_attribute_h_attribute_h_p\"),\n",
    "    (\"person_prefer_h_3_attribute_h_p\" , \"contents_attribute_h_attribute_h_p\"),\n",
    "\n",
    "]\n",
    "#########################################################################################################\n",
    "#######################앞에서 선택한 모델 학습법에 따라서 3개의 line중 적합한 것을 주석 해제하여 사용####################\n",
    "########################################################################################################\n",
    "\n",
    "# 학습에 필요없는 컬럼 리스트\n",
    "#cols_drop = [\"id_x\",\"person_prefer_f\",\"person_prefer_g\", \"contents_rn\"] #FE 사용할 경우\n",
    "#cols_drop = [\"id\",\"person_prefer_f\",\"person_prefer_g\"] #content_rn을 통으로 넣을 경우\n",
    "cols_drop = [\"id_x\",\"person_prefer_f\",\"person_prefer_g\", \"person_rn\", \"contents_rn\"] #FE2를 사용할 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((501951, 68), (501951,), (46404, 68))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, y_train = Preprocess_data(train_df, cols_merge = cols_merge , cols_equi= cols_equi , cols_drop = cols_drop)\n",
    "x_test, _ = Preprocess_data(test_df,is_train = False, cols_merge = cols_merge , cols_equi= cols_equi  , cols_drop = cols_drop)\n",
    "x_train.shape , y_train.shape , x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = x_train.columns[x_train.nunique() > 2].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_holdout = False\n",
    "n_splits = 5\n",
    "iterations = 3000\n",
    "patience = 100\n",
    "SEED = 42\n",
    "\n",
    "cv = KFold(n_splits = n_splits, shuffle = True, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "1\n",
      "Learning rate set to 0.027144\n",
      "0:\tlearn: 0.6154810\ttest: 0.6146935\tbest: 0.6146935 (0)\ttotal: 40.6ms\tremaining: 2m 1s\n",
      "100:\tlearn: 0.6374897\ttest: 0.6419290\tbest: 0.6419290 (100)\ttotal: 3.81s\tremaining: 1m 49s\n",
      "200:\tlearn: 0.6501011\ttest: 0.6563181\tbest: 0.6563181 (200)\ttotal: 7.44s\tremaining: 1m 43s\n",
      "300:\tlearn: 0.6572426\ttest: 0.6637949\tbest: 0.6637949 (300)\ttotal: 11.1s\tremaining: 1m 39s\n",
      "400:\tlearn: 0.6615530\ttest: 0.6683538\tbest: 0.6684557 (399)\ttotal: 14.7s\tremaining: 1m 35s\n",
      "500:\tlearn: 0.6650489\ttest: 0.6735410\tbest: 0.6736552 (497)\ttotal: 18.3s\tremaining: 1m 31s\n",
      "600:\tlearn: 0.6675269\ttest: 0.6766457\tbest: 0.6766457 (600)\ttotal: 21.8s\tremaining: 1m 27s\n",
      "700:\tlearn: 0.6694177\ttest: 0.6796537\tbest: 0.6797056 (697)\ttotal: 25.3s\tremaining: 1m 23s\n",
      "800:\tlearn: 0.6712148\ttest: 0.6813233\tbest: 0.6813233 (800)\ttotal: 28.8s\tremaining: 1m 19s\n",
      "900:\tlearn: 0.6726625\ttest: 0.6830009\tbest: 0.6830488 (898)\ttotal: 32.3s\tremaining: 1m 15s\n",
      "1000:\tlearn: 0.6740230\ttest: 0.6836120\tbest: 0.6836949 (997)\ttotal: 35.9s\tremaining: 1m 11s\n",
      "1100:\tlearn: 0.6750662\ttest: 0.6843905\tbest: 0.6843905 (1100)\ttotal: 39.4s\tremaining: 1m 7s\n",
      "1200:\tlearn: 0.6761274\ttest: 0.6852335\tbest: 0.6853443 (1184)\ttotal: 42.9s\tremaining: 1m 4s\n",
      "1300:\tlearn: 0.6767611\ttest: 0.6855445\tbest: 0.6855822 (1295)\ttotal: 46.4s\tremaining: 1m\n",
      "1400:\tlearn: 0.6775452\ttest: 0.6860015\tbest: 0.6861287 (1396)\ttotal: 49.9s\tremaining: 57s\n",
      "1500:\tlearn: 0.6782771\ttest: 0.6861083\tbest: 0.6861783 (1498)\ttotal: 53.4s\tremaining: 53.4s\n",
      "bestTest = 0.6861783019\n",
      "bestIteration = 1498\n",
      "Shrink model to first 1499 iterations.\n",
      "==================================================\n",
      "2\n",
      "Learning rate set to 0.027144\n",
      "0:\tlearn: 0.6110204\ttest: 0.6043188\tbest: 0.6043188 (0)\ttotal: 44.5ms\tremaining: 2m 13s\n",
      "100:\tlearn: 0.6370410\ttest: 0.6408162\tbest: 0.6408162 (100)\ttotal: 3.75s\tremaining: 1m 47s\n",
      "200:\tlearn: 0.6490334\ttest: 0.6561362\tbest: 0.6561492 (199)\ttotal: 7.39s\tremaining: 1m 42s\n",
      "300:\tlearn: 0.6558018\ttest: 0.6663157\tbest: 0.6663733 (299)\ttotal: 11s\tremaining: 1m 38s\n",
      "400:\tlearn: 0.6606933\ttest: 0.6717042\tbest: 0.6717042 (400)\ttotal: 14.6s\tremaining: 1m 34s\n",
      "500:\tlearn: 0.6638982\ttest: 0.6749431\tbest: 0.6749552 (495)\ttotal: 18.2s\tremaining: 1m 30s\n",
      "600:\tlearn: 0.6668431\ttest: 0.6782307\tbest: 0.6782307 (600)\ttotal: 21.8s\tremaining: 1m 26s\n",
      "700:\tlearn: 0.6689416\ttest: 0.6801833\tbest: 0.6801833 (700)\ttotal: 25.3s\tremaining: 1m 22s\n",
      "800:\tlearn: 0.6706618\ttest: 0.6815730\tbest: 0.6816609 (798)\ttotal: 28.8s\tremaining: 1m 19s\n",
      "900:\tlearn: 0.6724076\ttest: 0.6836586\tbest: 0.6837128 (897)\ttotal: 32.3s\tremaining: 1m 15s\n",
      "1000:\tlearn: 0.6735900\ttest: 0.6852136\tbest: 0.6852827 (996)\ttotal: 35.8s\tremaining: 1m 11s\n",
      "1100:\tlearn: 0.6746221\ttest: 0.6858379\tbest: 0.6859876 (1085)\ttotal: 39.2s\tremaining: 1m 7s\n",
      "1200:\tlearn: 0.6754233\ttest: 0.6866277\tbest: 0.6867278 (1192)\ttotal: 42.6s\tremaining: 1m 3s\n",
      "1300:\tlearn: 0.6762410\ttest: 0.6870968\tbest: 0.6872499 (1277)\ttotal: 46.1s\tremaining: 1m\n",
      "1400:\tlearn: 0.6770745\ttest: 0.6876411\tbest: 0.6876738 (1389)\ttotal: 49.6s\tremaining: 56.7s\n",
      "1500:\tlearn: 0.6778583\ttest: 0.6881229\tbest: 0.6881270 (1498)\ttotal: 53.1s\tremaining: 53.1s\n",
      "1600:\tlearn: 0.6785186\ttest: 0.6882587\tbest: 0.6883428 (1590)\ttotal: 56.7s\tremaining: 49.5s\n",
      "1700:\tlearn: 0.6790662\ttest: 0.6882620\tbest: 0.6883783 (1689)\ttotal: 1m\tremaining: 46s\n",
      "1800:\tlearn: 0.6796654\ttest: 0.6888821\tbest: 0.6888821 (1800)\ttotal: 1m 3s\tremaining: 42.4s\n",
      "1900:\tlearn: 0.6803321\ttest: 0.6891599\tbest: 0.6891599 (1900)\ttotal: 1m 7s\tremaining: 38.9s\n",
      "2000:\tlearn: 0.6809084\ttest: 0.6891354\tbest: 0.6891729 (1999)\ttotal: 1m 10s\tremaining: 35.4s\n",
      "2100:\tlearn: 0.6814516\ttest: 0.6890581\tbest: 0.6893295 (2058)\ttotal: 1m 14s\tremaining: 31.8s\n",
      "bestTest = 0.6893294661\n",
      "bestIteration = 2058\n",
      "Shrink model to first 2059 iterations.\n",
      "==================================================\n",
      "3\n",
      "Learning rate set to 0.027144\n",
      "0:\tlearn: 0.6146135\ttest: 0.6137830\tbest: 0.6137830 (0)\ttotal: 38ms\tremaining: 1m 53s\n",
      "100:\tlearn: 0.6377915\ttest: 0.6389286\tbest: 0.6391661 (99)\ttotal: 3.74s\tremaining: 1m 47s\n",
      "200:\tlearn: 0.6506638\ttest: 0.6518262\tbest: 0.6518387 (199)\ttotal: 7.4s\tremaining: 1m 43s\n",
      "300:\tlearn: 0.6567927\ttest: 0.6601821\tbest: 0.6601821 (300)\ttotal: 11s\tremaining: 1m 39s\n",
      "400:\tlearn: 0.6617050\ttest: 0.6663028\tbest: 0.6664433 (399)\ttotal: 14.7s\tremaining: 1m 34s\n",
      "500:\tlearn: 0.6651394\ttest: 0.6706463\tbest: 0.6706463 (500)\ttotal: 18.2s\tremaining: 1m 30s\n",
      "600:\tlearn: 0.6675725\ttest: 0.6737106\tbest: 0.6737169 (599)\ttotal: 21.9s\tremaining: 1m 27s\n",
      "700:\tlearn: 0.6696906\ttest: 0.6760617\tbest: 0.6760806 (699)\ttotal: 25.4s\tremaining: 1m 23s\n",
      "800:\tlearn: 0.6716363\ttest: 0.6777998\tbest: 0.6778361 (798)\ttotal: 28.9s\tremaining: 1m 19s\n",
      "900:\tlearn: 0.6731213\ttest: 0.6792673\tbest: 0.6792735 (899)\ttotal: 32.4s\tremaining: 1m 15s\n",
      "1000:\tlearn: 0.6744001\ttest: 0.6805760\tbest: 0.6806065 (999)\ttotal: 35.9s\tremaining: 1m 11s\n",
      "1100:\tlearn: 0.6753549\ttest: 0.6811489\tbest: 0.6814105 (1070)\ttotal: 39.4s\tremaining: 1m 7s\n",
      "1200:\tlearn: 0.6764174\ttest: 0.6822454\tbest: 0.6822454 (1200)\ttotal: 42.9s\tremaining: 1m 4s\n",
      "1300:\tlearn: 0.6770362\ttest: 0.6828209\tbest: 0.6828765 (1297)\ttotal: 46.4s\tremaining: 1m\n",
      "1400:\tlearn: 0.6778290\ttest: 0.6829959\tbest: 0.6830990 (1368)\ttotal: 49.9s\tremaining: 56.9s\n",
      "1500:\tlearn: 0.6785825\ttest: 0.6832925\tbest: 0.6832951 (1446)\ttotal: 53.3s\tremaining: 53.3s\n",
      "1600:\tlearn: 0.6793027\ttest: 0.6834796\tbest: 0.6836859 (1588)\ttotal: 56.9s\tremaining: 49.7s\n",
      "1700:\tlearn: 0.6799527\ttest: 0.6836847\tbest: 0.6837572 (1697)\ttotal: 1m\tremaining: 46.2s\n",
      "1800:\tlearn: 0.6806372\ttest: 0.6839140\tbest: 0.6840517 (1789)\ttotal: 1m 4s\tremaining: 42.6s\n",
      "1900:\tlearn: 0.6813744\ttest: 0.6841954\tbest: 0.6843417 (1898)\ttotal: 1m 7s\tremaining: 39.1s\n",
      "2000:\tlearn: 0.6818751\ttest: 0.6844069\tbest: 0.6845491 (1984)\ttotal: 1m 11s\tremaining: 35.5s\n",
      "2100:\tlearn: 0.6825821\ttest: 0.6846894\tbest: 0.6848513 (2080)\ttotal: 1m 14s\tremaining: 31.9s\n",
      "bestTest = 0.6848513341\n",
      "bestIteration = 2080\n",
      "Shrink model to first 2081 iterations.\n",
      "==================================================\n",
      "4\n",
      "Learning rate set to 0.027144\n",
      "0:\tlearn: 0.6145395\ttest: 0.6142310\tbest: 0.6142310 (0)\ttotal: 38.5ms\tremaining: 1m 55s\n",
      "100:\tlearn: 0.6365822\ttest: 0.6365856\tbest: 0.6365856 (100)\ttotal: 3.77s\tremaining: 1m 48s\n",
      "200:\tlearn: 0.6499580\ttest: 0.6515546\tbest: 0.6515546 (200)\ttotal: 7.5s\tremaining: 1m 44s\n",
      "300:\tlearn: 0.6573378\ttest: 0.6615626\tbest: 0.6615626 (300)\ttotal: 11.1s\tremaining: 1m 39s\n",
      "400:\tlearn: 0.6616500\ttest: 0.6673651\tbest: 0.6674162 (399)\ttotal: 14.7s\tremaining: 1m 35s\n",
      "500:\tlearn: 0.6643911\ttest: 0.6708484\tbest: 0.6708484 (500)\ttotal: 18.3s\tremaining: 1m 31s\n",
      "600:\tlearn: 0.6671966\ttest: 0.6740367\tbest: 0.6741062 (599)\ttotal: 21.9s\tremaining: 1m 27s\n",
      "700:\tlearn: 0.6691477\ttest: 0.6757725\tbest: 0.6758112 (699)\ttotal: 25.4s\tremaining: 1m 23s\n",
      "800:\tlearn: 0.6710469\ttest: 0.6779668\tbest: 0.6780292 (798)\ttotal: 28.9s\tremaining: 1m 19s\n",
      "900:\tlearn: 0.6723237\ttest: 0.6791862\tbest: 0.6793888 (894)\ttotal: 32.4s\tremaining: 1m 15s\n",
      "1000:\tlearn: 0.6736623\ttest: 0.6804787\tbest: 0.6805740 (993)\ttotal: 35.9s\tremaining: 1m 11s\n",
      "1100:\tlearn: 0.6747422\ttest: 0.6817527\tbest: 0.6817649 (1099)\ttotal: 39.5s\tremaining: 1m 8s\n",
      "1200:\tlearn: 0.6754907\ttest: 0.6821026\tbest: 0.6821284 (1199)\ttotal: 42.9s\tremaining: 1m 4s\n",
      "1300:\tlearn: 0.6763764\ttest: 0.6826467\tbest: 0.6827190 (1289)\ttotal: 46.4s\tremaining: 1m\n",
      "1400:\tlearn: 0.6771986\ttest: 0.6832149\tbest: 0.6832149 (1400)\ttotal: 50s\tremaining: 57.1s\n",
      "1500:\tlearn: 0.6779734\ttest: 0.6832620\tbest: 0.6833609 (1454)\ttotal: 53.6s\tremaining: 53.5s\n",
      "1600:\tlearn: 0.6786046\ttest: 0.6835085\tbest: 0.6835874 (1594)\ttotal: 57.1s\tremaining: 49.9s\n",
      "1700:\tlearn: 0.6794818\ttest: 0.6838756\tbest: 0.6840252 (1659)\ttotal: 1m\tremaining: 46.3s\n",
      "1800:\tlearn: 0.6801496\ttest: 0.6837365\tbest: 0.6841467 (1730)\ttotal: 1m 4s\tremaining: 42.7s\n",
      "bestTest = 0.6841466703\n",
      "bestIteration = 1730\n",
      "Shrink model to first 1731 iterations.\n",
      "==================================================\n",
      "5\n",
      "Learning rate set to 0.027144\n",
      "0:\tlearn: 0.6041014\ttest: 0.6008977\tbest: 0.6008977 (0)\ttotal: 39.9ms\tremaining: 1m 59s\n",
      "100:\tlearn: 0.6387948\ttest: 0.6387774\tbest: 0.6387774 (100)\ttotal: 3.77s\tremaining: 1m 48s\n",
      "200:\tlearn: 0.6516227\ttest: 0.6547896\tbest: 0.6547896 (200)\ttotal: 7.43s\tremaining: 1m 43s\n",
      "300:\tlearn: 0.6587149\ttest: 0.6626948\tbest: 0.6626948 (300)\ttotal: 11.1s\tremaining: 1m 39s\n",
      "400:\tlearn: 0.6630538\ttest: 0.6662309\tbest: 0.6662696 (397)\ttotal: 14.6s\tremaining: 1m 34s\n",
      "500:\tlearn: 0.6661684\ttest: 0.6693543\tbest: 0.6695404 (494)\ttotal: 18.2s\tremaining: 1m 30s\n",
      "600:\tlearn: 0.6692021\ttest: 0.6734539\tbest: 0.6734539 (600)\ttotal: 21.7s\tremaining: 1m 26s\n",
      "700:\tlearn: 0.6709127\ttest: 0.6754737\tbest: 0.6754991 (696)\ttotal: 25.3s\tremaining: 1m 22s\n",
      "800:\tlearn: 0.6722825\ttest: 0.6768740\tbest: 0.6769636 (794)\ttotal: 28.9s\tremaining: 1m 19s\n",
      "900:\tlearn: 0.6740160\ttest: 0.6788145\tbest: 0.6788145 (900)\ttotal: 32.4s\tremaining: 1m 15s\n",
      "1000:\tlearn: 0.6753364\ttest: 0.6801674\tbest: 0.6801674 (1000)\ttotal: 35.9s\tremaining: 1m 11s\n",
      "1100:\tlearn: 0.6762263\ttest: 0.6809245\tbest: 0.6810522 (1065)\ttotal: 39.4s\tremaining: 1m 7s\n",
      "1200:\tlearn: 0.6770986\ttest: 0.6816586\tbest: 0.6816834 (1186)\ttotal: 42.9s\tremaining: 1m 4s\n",
      "1300:\tlearn: 0.6779552\ttest: 0.6819374\tbest: 0.6820948 (1261)\ttotal: 46.4s\tremaining: 1m\n",
      "1400:\tlearn: 0.6788101\ttest: 0.6829058\tbest: 0.6829058 (1400)\ttotal: 49.9s\tremaining: 57s\n",
      "1500:\tlearn: 0.6795919\ttest: 0.6828912\tbest: 0.6831131 (1404)\ttotal: 53.4s\tremaining: 53.4s\n",
      "bestTest = 0.6831131197\n",
      "bestIteration = 1404\n",
      "Shrink model to first 1405 iterations.\n",
      "EOT\n"
     ]
    }
   ],
   "source": [
    "#Train\n",
    "\n",
    "scores = []\n",
    "models = []\n",
    "epochs = 1\n",
    "\n",
    "for train, validation in cv.split(x_train):\n",
    "    print(\"=====\"*10)\n",
    "    print(epochs)\n",
    "    epochs += 1\n",
    "    preds = []\n",
    "\n",
    "    model = CatBoostClassifier(\n",
    "        iterations, random_state = SEED, task_type = \"GPU\", eval_metric = 'F1',\n",
    "        cat_features = cat_features, one_hot_max_size = 5\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        x_train.iloc[train], y_train[train],\n",
    "        eval_set = [(x_train.iloc[validation], y_train[validation])],\n",
    "        early_stopping_rounds = patience, #########################무엇?\n",
    "        verbose = 100 #################무엇?\n",
    "    )\n",
    "\n",
    "    models.append(model)\n",
    "    scores.append(model.get_best_score()['validation']['F1'])\n",
    "\n",
    "    if is_holdout:\n",
    "        break\n",
    "\n",
    "print(\"EOT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6861783019138619, 0.6893294660916689, 0.684851334107285, 0.684146670261526, 0.6831131197405413]\n"
     ]
    }
   ],
   "source": [
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id_y', 'id', 'd_l_match_yn', 'contents_attribute_j_1',\n",
      "       'contents_attribute_h', 'd_m_match_yn',\n",
      "       'person_prefer_d_1_attribute_d_s', 'contents_attribute_d',\n",
      "       'contents_attribute_l', 'person_attribute_a_1'],\n",
      "      dtype='object')\n",
      "Index(['id_y', 'id', 'd_l_match_yn', 'contents_attribute_j_1',\n",
      "       'contents_attribute_h', 'd_m_match_yn',\n",
      "       'person_prefer_d_1_attribute_d_s', 'contents_attribute_d',\n",
      "       'contents_attribute_l', 'person_attribute_a_1'],\n",
      "      dtype='object')\n",
      "Index(['id_y', 'id', 'd_l_match_yn', 'contents_attribute_j_1', 'd_m_match_yn',\n",
      "       'contents_attribute_h', 'contents_attribute_d', 'contents_attribute_l',\n",
      "       'person_attribute_a_1', 'contents_attribute_h_attribute_h_p'],\n",
      "      dtype='object')\n",
      "Index(['id_y', 'id', 'd_l_match_yn', 'contents_attribute_j_1', 'd_m_match_yn',\n",
      "       'contents_attribute_h', 'person_prefer_d_1_attribute_d_s',\n",
      "       'contents_attribute_d', 'contents_attribute_l', 'person_attribute_a_1'],\n",
      "      dtype='object')\n",
      "Index(['id_y', 'id', 'd_l_match_yn', 'contents_attribute_j_1',\n",
      "       'contents_attribute_h', 'contents_attribute_d', 'd_m_match_yn',\n",
      "       'person_prefer_d_1_attribute_d_s', 'contents_attribute_l',\n",
      "       'person_attribute_a_1'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#feature importance\n",
    "\n",
    "for model in models:\n",
    "    idx = model.get_feature_importance().argsort()[-10:][::-1]\n",
    "    print(x_train.columns[idx])\n",
    "\n",
    "\n",
    "### id_y  = contetent_rn을 count로 변환한 값\n",
    "### id = person_rn을 count로 변환한 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38 : 0.7155805381663463 \n",
      "0.4 : 0.7150206515670943 \n",
      "0.42000000000000004 : 0.7141381103314193 \n",
      "0.44000000000000006 : 0.7127583237087429 \n",
      "0.4600000000000001 : 0.7107965973393628 \n"
     ]
    }
   ],
   "source": [
    "#Find Best Threshold\n",
    "\n",
    "pred_list = []\n",
    "scores = []\n",
    "\n",
    "thresholds = np.arange(0.38, 0.48, 0.02)\n",
    "for threshold in thresholds:\n",
    "    for i, (train, validation) in enumerate(cv.split(x_train)):\n",
    "        pred = models[i].predict_proba(x_train.iloc[validation])[:,1]\n",
    "        pred = np.where(pred >= threshold, 1, 0)\n",
    "        score = f1_score(y_train[validation], pred)\n",
    "        scores.append(score)\n",
    "        pred = models[i].predict_proba(x_test)[:,1]\n",
    "        pred_list.append(pred)\n",
    "    #print(scores)\n",
    "    print(f\"{threshold} : {np.mean(scores)} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.39243167, 0.25877795, 0.43555202, ..., 0.60569472, 0.58652873,\n",
       "       0.60906636])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold = 0.4\n",
    "pred = np.mean(pred_list, axis = 0)\n",
    "#pred = np.where(pred >= threshold, 1, 0)\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##########LGBM이랑 섞어보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm = pd.read_pickle(\"lgbm.pkl\")\n",
    "\n",
    "final = []\n",
    "for i in range(len(pred)):\n",
    "    final.append(pred[i] + lgbm[i] / 2)\n",
    "\n",
    "#final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5602022445898718,\n",
       " 0.4516975413816898,\n",
       " 0.6619100513948872,\n",
       " 0.621188899487424,\n",
       " 0.6400756496959005,\n",
       " 0.334215439773191,\n",
       " 0.8585116739036435,\n",
       " 0.2698879774189136,\n",
       " 0.7564118492077208,\n",
       " 0.5235040177085719,\n",
       " 0.8392127317142366,\n",
       " 0.8396927723248798,\n",
       " 0.5547785579637177,\n",
       " 0.789555706936645,\n",
       " 0.6922646497708512,\n",
       " 0.9451327924374218,\n",
       " 0.1304575737146768,\n",
       " 0.8447576396009058,\n",
       " 0.4788214495338625,\n",
       " 0.5932008097939659,\n",
       " 0.32205783696779666,\n",
       " 0.6045800260228729,\n",
       " 0.794084958251341,\n",
       " 0.9491866350127558,\n",
       " 0.8170479645121291,\n",
       " 0.5728894389126775,\n",
       " 0.6074506869029922,\n",
       " 0.8054211329895675,\n",
       " 0.7985919550310934,\n",
       " 0.7194020676154536,\n",
       " 0.8824087307516252,\n",
       " 1.008873229479982,\n",
       " 0.8658957599099162,\n",
       " 0.482989325335424,\n",
       " 0.5090579550317429,\n",
       " 0.7376610816864754,\n",
       " 0.07834390267878244,\n",
       " 0.9455350923138222,\n",
       " 0.7496053215127285,\n",
       " 0.42005989784756864,\n",
       " 0.7068351761016448,\n",
       " 0.6366157562742508,\n",
       " 0.9776880292567667,\n",
       " 0.9775684642101851,\n",
       " 1.1512547807919637,\n",
       " 0.9697001749564413,\n",
       " 0.8466266338103913,\n",
       " 0.7878254755628512,\n",
       " 0.711565571657714,\n",
       " 0.5829537714819167,\n",
       " 0.7549578663741741,\n",
       " 0.25848393592487173,\n",
       " 1.0095421059316878,\n",
       " 0.8995465200704378,\n",
       " 0.7315068259233946,\n",
       " 1.2194609817344249,\n",
       " 0.9702890620727367,\n",
       " 0.9465710115095832,\n",
       " 0.5755283822029008,\n",
       " 0.3695465955705236,\n",
       " 0.7021574335832251,\n",
       " 0.9328417720616371,\n",
       " 0.81002386971599,\n",
       " 1.169494033742847,\n",
       " 0.4936335156731484,\n",
       " 0.366184988112674,\n",
       " 0.31632057296872007,\n",
       " 0.9149753838171201,\n",
       " 0.8758355628098563,\n",
       " 0.8798413312683511,\n",
       " 0.7102001919428913,\n",
       " 0.9773798137250111,\n",
       " 0.7658398968462847,\n",
       " 0.7163107333002592,\n",
       " 0.9387588479568789,\n",
       " 0.9528520216075494,\n",
       " 0.5980962350694623,\n",
       " 0.7237202671949143,\n",
       " 0.4263478726618605,\n",
       " 0.693579761821028,\n",
       " 0.8108905835583126,\n",
       " 0.1447699634236871,\n",
       " 0.7456677873606549,\n",
       " 0.14753311449647524,\n",
       " 0.5377448050746435,\n",
       " 0.7473600881696942,\n",
       " 0.971757846117451,\n",
       " 0.9875440029249146,\n",
       " 1.0450838249363226,\n",
       " 0.7953518765557728,\n",
       " 0.7554192108046751,\n",
       " 0.9788177431640004,\n",
       " 0.8721612747574576,\n",
       " 0.42659764017536245,\n",
       " 0.09804659766228627,\n",
       " 0.770601146494788,\n",
       " 0.7539165207536245,\n",
       " 0.3833744095747996,\n",
       " 0.15060645226377117,\n",
       " 0.5566620318597014,\n",
       " 1.1707055982207724,\n",
       " 0.7848493686388429,\n",
       " 0.38850319577308373,\n",
       " 0.633010605400784,\n",
       " 0.4124051304080261,\n",
       " 0.5758910328208623,\n",
       " 0.6074866879412064,\n",
       " 0.1777662702731996,\n",
       " 0.9971287978178605,\n",
       " 0.6925162660190232,\n",
       " 0.530344610597085,\n",
       " 0.7052423086541114,\n",
       " 0.9106231185466682,\n",
       " 0.9255252635404809,\n",
       " 0.6701566844786349,\n",
       " 0.44808942614243463,\n",
       " 0.7461026251783779,\n",
       " 0.6824726675354241,\n",
       " 0.3691089210811128,\n",
       " 0.9911412685165515,\n",
       " 0.8229983734500244,\n",
       " 0.8023949885845324,\n",
       " 0.9291257498866279,\n",
       " 0.3694321485856018,\n",
       " 0.952736228965181,\n",
       " 1.0225832950958955,\n",
       " 0.7204541600894142,\n",
       " 0.8586422662349731,\n",
       " 0.842205846907333,\n",
       " 0.7252326086650844,\n",
       " 0.6204190500157285,\n",
       " 0.7258983937971736,\n",
       " 0.22145191564529407,\n",
       " 0.8544029614929862,\n",
       " 0.7223772041531018,\n",
       " 1.0264442108992589,\n",
       " 1.1484495309307075,\n",
       " 1.1074907506220881,\n",
       " 0.732861779144762,\n",
       " 0.25201592797399697,\n",
       " 0.7590438862641122,\n",
       " 0.767611160019426,\n",
       " 0.9250876451923636,\n",
       " 0.5303591086187298,\n",
       " 0.927917322386167,\n",
       " 0.876431425094705,\n",
       " 0.7618927362179029,\n",
       " 1.1869633094428689,\n",
       " 0.9092022024586045,\n",
       " 0.9755426860166534,\n",
       " 0.7289553361514491,\n",
       " 0.08645402279763378,\n",
       " 0.901223999929445,\n",
       " 0.6979385386911657,\n",
       " 0.8055043381339027,\n",
       " 0.5812159903392584,\n",
       " 0.9168488763932192,\n",
       " 0.47446875208919315,\n",
       " 0.6221498629924501,\n",
       " 0.35906285637330104,\n",
       " 0.9409939235353795,\n",
       " 0.3505205485687812,\n",
       " 0.5226289077539195,\n",
       " 0.3359349775520978,\n",
       " 0.08287364968355054,\n",
       " 0.19305156148861985,\n",
       " 0.06138462690274071,\n",
       " 1.1569170918877323,\n",
       " 0.5532132559521952,\n",
       " 0.898524832710774,\n",
       " 0.7027537252612133,\n",
       " 0.7907415138359797,\n",
       " 0.9577098407882196,\n",
       " 0.9225251790293421,\n",
       " 0.5914241472484594,\n",
       " 0.878911686689439,\n",
       " 0.8788121372365805,\n",
       " 0.7738931207380941,\n",
       " 0.8335440514043384,\n",
       " 0.4750664368356928,\n",
       " 0.8531180874686481,\n",
       " 0.8677234925672466,\n",
       " 1.0581466371211863,\n",
       " 0.4319641955579311,\n",
       " 0.20327224597055968,\n",
       " 0.20057085989671306,\n",
       " 0.47092238127642316,\n",
       " 0.7140628542020595,\n",
       " 0.6179290275213236,\n",
       " 0.7158997571660457,\n",
       " 0.9310785227354175,\n",
       " 0.5673041485599938,\n",
       " 0.6459033429435784,\n",
       " 0.6523320890621787,\n",
       " 0.813885191164257,\n",
       " 0.7374831599408376,\n",
       " 0.34948206032386475,\n",
       " 0.6089058966434912,\n",
       " 1.0036121533448894,\n",
       " 0.5002513841811048,\n",
       " 0.45486305089052903,\n",
       " 0.9887180211490718,\n",
       " 0.8155866368843036,\n",
       " 0.8443690689306976,\n",
       " 1.0338292761650927,\n",
       " 1.0334431377979574,\n",
       " 0.7476886433684431,\n",
       " 0.6362462628219472,\n",
       " 0.7195791820814121,\n",
       " 0.37385879019778173,\n",
       " 0.9217539556421241,\n",
       " 0.49311271288207204,\n",
       " 1.1227115081709935,\n",
       " 0.6933696115757122,\n",
       " 0.8223871373058622,\n",
       " 0.8464192407958706,\n",
       " 0.6659078931694855,\n",
       " 0.7433460997033974,\n",
       " 0.5926010549934408,\n",
       " 0.7911712240081803,\n",
       " 0.7607729689453366,\n",
       " 0.950555143334368,\n",
       " 0.9500087249375444,\n",
       " 0.6854858326115666,\n",
       " 0.46919330161761835,\n",
       " 0.7742241666514056,\n",
       " 0.701117852290938,\n",
       " 0.22872980887320876,\n",
       " 0.4310209655307199,\n",
       " 0.9466362877439949,\n",
       " 0.7624182296851871,\n",
       " 0.3933471212137246,\n",
       " 1.109924764646722,\n",
       " 0.4766179813265823,\n",
       " 0.4936383793946524,\n",
       " 0.36746267841340424,\n",
       " 0.5427282289865283,\n",
       " 0.6009033315098804,\n",
       " 0.9702664985693189,\n",
       " 0.3141400049779273,\n",
       " 0.7858453351759477,\n",
       " 0.8801777484053581,\n",
       " 0.6995117856894466,\n",
       " 0.6120374831666728,\n",
       " 0.3513446959893709,\n",
       " 0.9046427936063418,\n",
       " 0.5639065503279668,\n",
       " 0.7726303533963257,\n",
       " 0.738019150792067,\n",
       " 0.8127625601603046,\n",
       " 0.7211589007448614,\n",
       " 0.9427833953961022,\n",
       " 0.9242942814244139,\n",
       " 0.42397166045641277,\n",
       " 1.0714282141419469,\n",
       " 0.6679857494146516,\n",
       " 0.9102579710388385,\n",
       " 1.0287497254723754,\n",
       " 1.1189057194680845,\n",
       " 0.9818453539646087,\n",
       " 0.9659784667990937,\n",
       " 0.8572357280295233,\n",
       " 0.9886481630118007,\n",
       " 1.0012625101488848,\n",
       " 1.1803763378616061,\n",
       " 0.23285106732679592,\n",
       " 0.6706921700532308,\n",
       " 0.8522642119428596,\n",
       " 1.0344347415273938,\n",
       " 0.7004558568254335,\n",
       " 1.05131212587512,\n",
       " 0.8355373614075843,\n",
       " 0.6146408550338802,\n",
       " 0.7694553693089613,\n",
       " 0.646913072865378,\n",
       " 0.6803526300077911,\n",
       " 0.6594853143596399,\n",
       " 0.6369158989617592,\n",
       " 0.20130922390936973,\n",
       " 0.8371410123446096,\n",
       " 0.9965608870083957,\n",
       " 0.6158015715799781,\n",
       " 0.7777620503809201,\n",
       " 1.0458159978012738,\n",
       " 0.4669973931715665,\n",
       " 0.7546787841865286,\n",
       " 0.3668684469815886,\n",
       " 0.990551733086538,\n",
       " 0.526306806915007,\n",
       " 0.7025895511386979,\n",
       " 0.9681273408331085,\n",
       " 0.7781057018671261,\n",
       " 0.6224965883901178,\n",
       " 0.7042986205592018,\n",
       " 0.959403422192113,\n",
       " 0.7788103648715063,\n",
       " 0.7543371344511833,\n",
       " 0.5709626024214045,\n",
       " 1.018713829776879,\n",
       " 0.733107807306008,\n",
       " 0.5233467587041642,\n",
       " 0.9514262968924796,\n",
       " 0.5464198383943211,\n",
       " 1.0337557852763803,\n",
       " 0.36494676361179634,\n",
       " 0.7173041761411096,\n",
       " 0.7661621652812446,\n",
       " 0.746360012525641,\n",
       " 1.0537928067193714,\n",
       " 0.46579635039641926,\n",
       " 0.8414358666509323,\n",
       " 0.5914261464501974,\n",
       " 0.8970336858716228,\n",
       " 0.8923985062353028,\n",
       " 0.6799669931732216,\n",
       " 0.4995510211251388,\n",
       " 0.649701126093744,\n",
       " 0.9763813437288787,\n",
       " 0.7521535201300744,\n",
       " 0.7062432791260987,\n",
       " 0.847613831462984,\n",
       " 0.47456857528213003,\n",
       " 0.5279921718583289,\n",
       " 0.4841601887672611,\n",
       " 0.5000750986639524,\n",
       " 0.25824041535667014,\n",
       " 1.0675401752767268,\n",
       " 0.8235586395799314,\n",
       " 0.7552741939713459,\n",
       " 0.6680369027548563,\n",
       " 0.5175467910256577,\n",
       " 0.819955988526359,\n",
       " 0.6534507413762247,\n",
       " 0.8616540822796298,\n",
       " 0.7640981952096222,\n",
       " 1.0538409899758365,\n",
       " 0.8502899161893369,\n",
       " 0.1580777702027204,\n",
       " 0.24807087278372902,\n",
       " 0.5297175289893137,\n",
       " 0.9023677012019596,\n",
       " 0.6292181017167064,\n",
       " 0.9469419717623089,\n",
       " 0.6917932202184364,\n",
       " 1.1195478172788782,\n",
       " 0.6840682330931303,\n",
       " 0.4170734608621227,\n",
       " 0.9563303129765413,\n",
       " 0.1411039742495769,\n",
       " 0.37357506660781886,\n",
       " 0.9841303231462539,\n",
       " 1.047593833256557,\n",
       " 0.6005523496943446,\n",
       " 0.9516736005353457,\n",
       " 0.5760307248618589,\n",
       " 0.5460539079444839,\n",
       " 0.4295015165683716,\n",
       " 0.9479031346293918,\n",
       " 0.6667564882367876,\n",
       " 0.4855254436932921,\n",
       " 0.7502286873850315,\n",
       " 0.6106013917469761,\n",
       " 0.8692820882702061,\n",
       " 1.0906988707971825,\n",
       " 1.0761280516312728,\n",
       " 1.0589117018687388,\n",
       " 0.47844859983384247,\n",
       " 0.603040602930099,\n",
       " 0.9082709108835096,\n",
       " 0.4774686631197306,\n",
       " 0.8198050387167635,\n",
       " 0.9980433644086203,\n",
       " 1.0539953502229025,\n",
       " 0.4548082754315833,\n",
       " 0.9855399648285721,\n",
       " 0.4853256404633402,\n",
       " 0.6135864742439842,\n",
       " 0.770024774637271,\n",
       " 0.665936377835424,\n",
       " 0.5149155661198945,\n",
       " 0.5340640139214061,\n",
       " 0.6031604213778112,\n",
       " 0.9681629920079708,\n",
       " 0.8905281570637895,\n",
       " 0.8760944761975611,\n",
       " 0.8894990474401024,\n",
       " 0.6776461311838646,\n",
       " 0.7321180872784212,\n",
       " 0.9183571863939846,\n",
       " 1.1019237217408593,\n",
       " 0.9579433450375767,\n",
       " 0.7955216241671323,\n",
       " 0.8704889583743786,\n",
       " 0.5089401106698628,\n",
       " 0.8679754955787962,\n",
       " 0.8040431110976116,\n",
       " 0.9566341570203183,\n",
       " 1.1829750943213482,\n",
       " 0.9384448756823269,\n",
       " 0.722230100669371,\n",
       " 0.5373555041238682,\n",
       " 0.9612810530157205,\n",
       " 0.8802317690333041,\n",
       " 1.050221147663041,\n",
       " 0.5801839759199849,\n",
       " 0.7259421180365585,\n",
       " 0.7425997410296603,\n",
       " 0.9296697158558049,\n",
       " 0.9878484243561058,\n",
       " 0.6494259253544437,\n",
       " 0.47045725559542606,\n",
       " 0.671192257618252,\n",
       " 0.47733635574959615,\n",
       " 0.5339226939128924,\n",
       " 0.4526214259153637,\n",
       " 0.5806056734710093,\n",
       " 0.7969160572192149,\n",
       " 0.24443862594190802,\n",
       " 1.062734533588456,\n",
       " 0.19942873006292372,\n",
       " 0.7350116075651107,\n",
       " 0.5820877349339699,\n",
       " 0.7030166573638127,\n",
       " 0.988242842348309,\n",
       " 1.012431580789508,\n",
       " 0.5112372806664159,\n",
       " 0.7444785577052684,\n",
       " 0.9308573992847176,\n",
       " 0.8989291253209739,\n",
       " 0.9968952848211625,\n",
       " 0.5054498165543888,\n",
       " 0.6844281480940284,\n",
       " 0.7750085670431087,\n",
       " 0.9705781871486373,\n",
       " 0.6968623860003027,\n",
       " 0.5463975591109821,\n",
       " 0.6196646903464463,\n",
       " 0.3953227784582651,\n",
       " 0.9115120924489029,\n",
       " 0.8027327879020117,\n",
       " 0.7733645557291277,\n",
       " 0.5862698028382333,\n",
       " 0.5353967686502545,\n",
       " 0.06344560262602536,\n",
       " 1.0471532401627157,\n",
       " 0.20879390068255715,\n",
       " 0.363730906708462,\n",
       " 0.7214903346900594,\n",
       " 0.8195736571035761,\n",
       " 0.22389833789038865,\n",
       " 1.0340267760974253,\n",
       " 0.6699888961224003,\n",
       " 0.855865344989966,\n",
       " 0.8492364424510985,\n",
       " 0.4547863037909456,\n",
       " 0.6099860324512112,\n",
       " 0.676319517360205,\n",
       " 0.7513301996354087,\n",
       " 0.8559262180946443,\n",
       " 0.7281354509156321,\n",
       " 0.6943344459970289,\n",
       " 1.0946093285241785,\n",
       " 0.36936451441190754,\n",
       " 0.4603693382197079,\n",
       " 0.39699524491333094,\n",
       " 0.7762289606152006,\n",
       " 0.7544375605028943,\n",
       " 0.6092936916949182,\n",
       " 0.6982464728274239,\n",
       " 0.7857511912560287,\n",
       " 0.6464009859253667,\n",
       " 0.6225841206231524,\n",
       " 0.8027523331388675,\n",
       " 0.6050283323489187,\n",
       " 0.5954548644813474,\n",
       " 1.1038112138883915,\n",
       " 0.32115039514524607,\n",
       " 1.0232561224871448,\n",
       " 0.38716880882841614,\n",
       " 1.0466595392224556,\n",
       " 0.9557139101605694,\n",
       " 0.7439671180698275,\n",
       " 0.67176932618612,\n",
       " 0.5079729327221232,\n",
       " 1.0175504809350553,\n",
       " 0.6916056625355594,\n",
       " 0.7996460172158888,\n",
       " 0.8157206877912746,\n",
       " 0.6042420083386423,\n",
       " 0.7306612792952392,\n",
       " 0.6336073770238451,\n",
       " 0.297197099240188,\n",
       " 0.6224837027269814,\n",
       " 0.749742644144813,\n",
       " 0.3060299001106143,\n",
       " 0.3307433557588809,\n",
       " 0.9722274162854346,\n",
       " 0.9024655863985191,\n",
       " 0.9031409379028157,\n",
       " 0.9319324962082861,\n",
       " 0.44091849102690717,\n",
       " 0.8809708363733992,\n",
       " 0.7346110078990822,\n",
       " 0.7825831413757619,\n",
       " 0.8261661274128298,\n",
       " 0.3641910876273575,\n",
       " 0.8832845342886022,\n",
       " 0.5659997139254869,\n",
       " 0.4490757579401704,\n",
       " 0.30680899926102956,\n",
       " 0.7688246928070127,\n",
       " 0.43789974262743037,\n",
       " 0.6543144040441655,\n",
       " 0.17701269917200427,\n",
       " 0.342510881116499,\n",
       " 0.7492650627351238,\n",
       " 0.6268555864759033,\n",
       " 0.8878646109711791,\n",
       " 1.024384628787515,\n",
       " 0.25322090795338204,\n",
       " 0.7218651101626875,\n",
       " 0.8177972166574567,\n",
       " 0.17049227658726995,\n",
       " 0.7910142431079477,\n",
       " 0.6662710339695893,\n",
       " 0.3052010970355097,\n",
       " 0.8423416259044447,\n",
       " 0.6425072031963562,\n",
       " 0.9106744500348939,\n",
       " 0.7076105610858554,\n",
       " 0.7587506656165928,\n",
       " 0.7977618719936086,\n",
       " 0.8518219290933535,\n",
       " 0.7075482010403247,\n",
       " 0.2949365917935258,\n",
       " 0.5518101523637581,\n",
       " 0.6059957321864352,\n",
       " 0.7448043198215809,\n",
       " 0.495232155520822,\n",
       " 0.8181140542864644,\n",
       " 0.7272915806861263,\n",
       " 0.8874122966283812,\n",
       " 1.0033239193617922,\n",
       " 0.8218240034978108,\n",
       " 0.5468747381680774,\n",
       " 0.4518152202854383,\n",
       " 1.011714459566743,\n",
       " 0.7345061890238107,\n",
       " 0.9795036500036416,\n",
       " 0.19082484920093495,\n",
       " 0.9416210395855495,\n",
       " 0.6675003642561744,\n",
       " 0.4936822582699001,\n",
       " 0.7717414757718982,\n",
       " 0.6967633874465602,\n",
       " 0.4840725878829084,\n",
       " 0.3641899873538158,\n",
       " 0.9570519851087691,\n",
       " 0.8530674818118684,\n",
       " 0.37096337098399745,\n",
       " 0.5878046819246855,\n",
       " 0.2068200060790133,\n",
       " 0.8172009934687323,\n",
       " 0.6502113084311616,\n",
       " 0.6405301601061544,\n",
       " 0.7179346633096934,\n",
       " 1.2194817436805734,\n",
       " 1.1897503388379735,\n",
       " 0.9320135199186229,\n",
       " 0.6363477222044417,\n",
       " 0.8600029211286688,\n",
       " 0.8094366705116942,\n",
       " 0.39838816045929504,\n",
       " 0.8358036996882652,\n",
       " 0.7345284657252339,\n",
       " 0.9219003038877316,\n",
       " 0.5109923537783158,\n",
       " 0.9125142480958731,\n",
       " 0.4226374108045764,\n",
       " 0.8765298363911649,\n",
       " 0.7835800580044977,\n",
       " 0.18749397511451363,\n",
       " 0.8737631045216189,\n",
       " 0.5275658320738787,\n",
       " 0.6364581968330786,\n",
       " 0.7381181761666592,\n",
       " 0.3816258712984397,\n",
       " 0.7138410907014582,\n",
       " 1.0425158120041274,\n",
       " 0.6765943866994915,\n",
       " 1.0363408667493086,\n",
       " 0.7749805582896487,\n",
       " 0.5108945789120458,\n",
       " 0.5553160570487925,\n",
       " 0.6726432862208571,\n",
       " 0.29785537716234933,\n",
       " 0.7251044011646702,\n",
       " 0.36632217792512073,\n",
       " 0.9140605636862517,\n",
       " 0.9470333923643997,\n",
       " 0.5488356741810471,\n",
       " 0.7933365843576414,\n",
       " 0.7594939798165821,\n",
       " 0.7845595756211687,\n",
       " 0.6116851662562639,\n",
       " 0.3884762684387949,\n",
       " 0.6736559646527703,\n",
       " 0.8000423030496493,\n",
       " 0.9850618613757185,\n",
       " 0.7101025841649315,\n",
       " 0.9845187326828977,\n",
       " 0.8192589212793012,\n",
       " 0.44091432895215105,\n",
       " 0.4339256896088626,\n",
       " 0.7281495445110566,\n",
       " 0.42093899244492833,\n",
       " 0.8500232681191005,\n",
       " 0.8671192530097539,\n",
       " 0.9178370121699811,\n",
       " 0.7742098195809679,\n",
       " 0.7692885593979496,\n",
       " 0.5829277596414812,\n",
       " 0.7049290060969452,\n",
       " 0.5760754727964777,\n",
       " 0.9748828171121044,\n",
       " 0.839623577454288,\n",
       " 0.536518350469933,\n",
       " 0.856439908334104,\n",
       " 1.10298848228704,\n",
       " 0.4301743105377238,\n",
       " 0.4277219071626241,\n",
       " 0.4420688137002466,\n",
       " 0.8677057949818308,\n",
       " 0.8566254199985545,\n",
       " 0.7516767628524735,\n",
       " 0.4867147834752418,\n",
       " 0.5400536304687721,\n",
       " 0.4092019692297836,\n",
       " 0.9598279753381196,\n",
       " 1.0207733945210185,\n",
       " 0.6388549861584103,\n",
       " 0.36967222651852294,\n",
       " 0.584401314986166,\n",
       " 0.7795990335547935,\n",
       " 0.9654456186917322,\n",
       " 0.7863274113662841,\n",
       " 0.6912545476634582,\n",
       " 0.7726359462031964,\n",
       " 0.6235197343601075,\n",
       " 0.8809281415980751,\n",
       " 0.857694746117001,\n",
       " 0.6945662807335179,\n",
       " 0.7085727024436415,\n",
       " 0.6555707394574484,\n",
       " 0.2531271875738822,\n",
       " 0.6175804351892111,\n",
       " 0.9302760999751409,\n",
       " 0.7084015910461781,\n",
       " 0.740779908505661,\n",
       " 0.729060685362636,\n",
       " 1.0298467392930624,\n",
       " 0.7992747553780208,\n",
       " 0.9982846159873335,\n",
       " 0.7163958707317967,\n",
       " 1.101059528651032,\n",
       " 1.0220172953583713,\n",
       " 0.9128008948524688,\n",
       " 0.6793870352521854,\n",
       " 0.6722040910795712,\n",
       " 1.0163047518607835,\n",
       " 0.665684587575919,\n",
       " 0.9976846278917931,\n",
       " 0.4735756556285551,\n",
       " 0.3331519224607672,\n",
       " 0.5762613238531706,\n",
       " 0.23323759155902848,\n",
       " 0.4153987398754724,\n",
       " 0.9320959449874182,\n",
       " 0.14580703847369839,\n",
       " 0.6700909875343335,\n",
       " 0.526386713512792,\n",
       " 0.13609794351536447,\n",
       " 0.7941181531540729,\n",
       " 0.6207703085414445,\n",
       " 0.9459907821441912,\n",
       " 1.064877660905028,\n",
       " 0.4563187890903173,\n",
       " 0.21464850923108353,\n",
       " 0.7451049801403792,\n",
       " 0.21596967923604815,\n",
       " 0.5121293356832008,\n",
       " 0.9404035574538547,\n",
       " 0.7090529904062948,\n",
       " 0.9027633341265661,\n",
       " 0.879507575295873,\n",
       " 1.074574498482707,\n",
       " 0.8846545770992786,\n",
       " 0.5910601235435067,\n",
       " 0.9611499098539167,\n",
       " 0.5372824976873875,\n",
       " 1.0255009353067739,\n",
       " 0.5337851000952865,\n",
       " 0.837257994022082,\n",
       " 1.040784720913189,\n",
       " 0.44707331340516787,\n",
       " 0.4553699438515031,\n",
       " 0.6341747318708679,\n",
       " 0.8674956266954672,\n",
       " 0.6335666913817232,\n",
       " 0.6860362922122757,\n",
       " 0.5354429327507981,\n",
       " 0.8634480175037575,\n",
       " 0.7984360221407505,\n",
       " 0.5884358308871757,\n",
       " 0.45192852929984795,\n",
       " 1.0143540603091803,\n",
       " 0.9363687808569462,\n",
       " 0.7527040824182734,\n",
       " 0.5747492921701979,\n",
       " 0.6748341466678704,\n",
       " 0.804835552320047,\n",
       " 0.5955930878677786,\n",
       " 0.8794560753128176,\n",
       " 0.0944215859126398,\n",
       " 0.8776112200333384,\n",
       " 0.8898825545210962,\n",
       " 0.479451172391258,\n",
       " 0.6624230006966452,\n",
       " 0.9443691678160671,\n",
       " 0.8368427325958772,\n",
       " 0.8299809139538569,\n",
       " 0.6294248103974682,\n",
       " 0.49303964350226337,\n",
       " 0.12395868055857184,\n",
       " 0.26588522941063186,\n",
       " 1.029926608685468,\n",
       " 0.518674774237244,\n",
       " 0.7903412783857943,\n",
       " 0.6187787150399855,\n",
       " 1.0709483606097925,\n",
       " 0.513435676093264,\n",
       " 0.3560764704689302,\n",
       " 0.17088027015011334,\n",
       " 1.104718444101093,\n",
       " 1.085940291233369,\n",
       " 0.8893338649492448,\n",
       " 0.6729981650610429,\n",
       " 0.9288366013737992,\n",
       " 0.7830038382690024,\n",
       " 0.7919168352961694,\n",
       " 0.5692197805284706,\n",
       " 0.5245428961488197,\n",
       " 0.4535083230430984,\n",
       " 0.45731436140617765,\n",
       " 0.9282583007285037,\n",
       " 0.7820014930027293,\n",
       " 0.7791207551722694,\n",
       " 0.7981633951473881,\n",
       " 0.14620393776204588,\n",
       " 0.6295620158113957,\n",
       " 0.8444537353260307,\n",
       " 0.8841118788526436,\n",
       " 0.8693416913620533,\n",
       " 0.6488090325708686,\n",
       " 0.9270227849916008,\n",
       " 1.1328590630390198,\n",
       " 0.8013454321572102,\n",
       " 0.605610778595864,\n",
       " 0.8412616877784769,\n",
       " 0.9910139652492809,\n",
       " 0.4502351023107287,\n",
       " 1.0920376941382541,\n",
       " 0.5374818195510451,\n",
       " 0.6694856981984147,\n",
       " 0.9558182378033142,\n",
       " 0.6996780716231317,\n",
       " 0.8567202253861158,\n",
       " 0.3525983102778089,\n",
       " 0.6572760402559217,\n",
       " 0.7050537555378273,\n",
       " 0.12438550601791758,\n",
       " 0.5095496892820061,\n",
       " 0.5937102561356468,\n",
       " 0.8159735839361257,\n",
       " 0.6965747756625182,\n",
       " 0.7401470153449399,\n",
       " 0.770910938044289,\n",
       " 0.6166745666891407,\n",
       " 0.7455807426448764,\n",
       " 0.6456173861187113,\n",
       " 0.689462605298855,\n",
       " 0.9755912038031627,\n",
       " 0.8509066099636723,\n",
       " 0.922789833228364,\n",
       " 1.0388205543599152,\n",
       " 0.2061103308996453,\n",
       " 0.9824870728503302,\n",
       " 1.2095113671007443,\n",
       " 0.8485641000169946,\n",
       " 0.8760502619231811,\n",
       " 0.7213094381523676,\n",
       " 0.7325442785415288,\n",
       " 0.7410352298838974,\n",
       " 1.0205860077555333,\n",
       " 0.5063013485931802,\n",
       " 0.8769005577775597,\n",
       " 0.7856218381929343,\n",
       " 1.0165792248705026,\n",
       " 0.8081041440131802,\n",
       " 0.5108154303363511,\n",
       " 0.7957914174554135,\n",
       " 0.578106298354791,\n",
       " 0.7711844772471289,\n",
       " 0.5053789442085678,\n",
       " 0.32989172059815935,\n",
       " 0.433657527878108,\n",
       " 0.8597144155354035,\n",
       " 0.758994929350399,\n",
       " 0.7652400203298982,\n",
       " 0.6834006786984819,\n",
       " 1.256423512053293,\n",
       " 0.7230181262150221,\n",
       " 1.139384427768577,\n",
       " 0.6707129651171926,\n",
       " 0.524382520640057,\n",
       " 1.0548092351708933,\n",
       " 0.1298379250512488,\n",
       " 0.4340310588183671,\n",
       " 0.5735287898614201,\n",
       " 0.8941874373375975,\n",
       " 0.3692313789400556,\n",
       " 0.5388050615998389,\n",
       " 0.7761530545967767,\n",
       " 0.7978715995388255,\n",
       " 0.6494913431758432,\n",
       " 0.7266926867740866,\n",
       " 0.5641533294351031,\n",
       " 0.9236832913714268,\n",
       " 0.7333374616365018,\n",
       " 0.5972535902550646,\n",
       " 0.5847794527626498,\n",
       " 0.7762554780445519,\n",
       " 0.4632709290882901,\n",
       " 0.9742962868240119,\n",
       " 0.6444940671783266,\n",
       " 0.8747965990067661,\n",
       " 1.057187491752284,\n",
       " 1.056161041975982,\n",
       " 1.002595591521119,\n",
       " 0.9751914040851829,\n",
       " 1.0075383944065024,\n",
       " 0.7688984600377546,\n",
       " 0.9593474663340156,\n",
       " 0.8801536842271518,\n",
       " 1.085354086422457,\n",
       " 0.9799072389406258,\n",
       " 0.6246706506727264,\n",
       " 0.8906459641787521,\n",
       " 1.053810050278613,\n",
       " 0.4795410972297801,\n",
       " 1.0856884130630218,\n",
       " 1.1932126360189281,\n",
       " 0.7349012514079392,\n",
       " 0.42305418306975306,\n",
       " 0.5360947010263641,\n",
       " 0.6436606293572134,\n",
       " 0.07127477674160314,\n",
       " 0.9407213403118924,\n",
       " 0.7363480357382619,\n",
       " 0.6795279804741299,\n",
       " 0.6281174959904587,\n",
       " 0.6111414307263172,\n",
       " 0.8675996856139153,\n",
       " 1.00434041124812,\n",
       " 0.696039685381299,\n",
       " 0.9138426339311139,\n",
       " 0.22621689398638706,\n",
       " 1.1187878837929168,\n",
       " 1.0526096545156407,\n",
       " 0.758962779332073,\n",
       " 0.7552381524875833,\n",
       " 0.7433849931532021,\n",
       " 1.0763729435941414,\n",
       " 0.2650822890972405,\n",
       " 0.8584515399827739,\n",
       " 0.3742859266033123,\n",
       " 0.3023144132436042,\n",
       " 0.8596230152874258,\n",
       " 0.7561906568842167,\n",
       " 0.8804665468807595,\n",
       " 0.8672314152771452,\n",
       " 0.962612157062706,\n",
       " 0.9462998424382986,\n",
       " 0.8080300857467734,\n",
       " 0.7769389280393768,\n",
       " 1.029514968040539,\n",
       " 1.0785187268378646,\n",
       " 0.536524277213392,\n",
       " 0.904247424076098,\n",
       " 0.36597548150976067,\n",
       " 0.4136003500010783,\n",
       " 0.5484194586519057,\n",
       " 0.5772293993230979,\n",
       " 0.7293765451095577,\n",
       " 0.8658078214594839,\n",
       " 0.8745977555347171,\n",
       " 0.941701364453174,\n",
       " 0.9965894491333648,\n",
       " 0.7523157595143657,\n",
       " 0.8729496368778751,\n",
       " 0.8469032133834269,\n",
       " 0.8578658146055398,\n",
       " 0.7783989723799407,\n",
       " 0.890891016696177,\n",
       " 0.6111173514786413,\n",
       " 0.6792226082375683,\n",
       " 0.6383177151511015,\n",
       " 0.6215292412471376,\n",
       " 0.7902556788539665,\n",
       " 0.7852533353912824,\n",
       " 0.6256940237762771,\n",
       " 0.9300777465200943,\n",
       " 0.31773653752845565,\n",
       " 0.6839791026758558,\n",
       " 0.8079099909696039,\n",
       " 0.558424352933778,\n",
       " 0.7862757161446319,\n",
       " 0.47820396322439807,\n",
       " 1.0618379889681884,\n",
       " 0.6376929882890372,\n",
       " 0.8951127375410924,\n",
       " 0.8938611173466829,\n",
       " 0.9756279970587136,\n",
       " 0.6780559886373076,\n",
       " 0.4515196884808668,\n",
       " 0.9449008687305253,\n",
       " 0.6347230973117739,\n",
       " 0.17535760968554817,\n",
       " 0.8492186737812181,\n",
       " 0.9677821866640908,\n",
       " 1.0744486236338604,\n",
       " 0.8109747700851662,\n",
       " 0.8625848363827047,\n",
       " 0.833658183989219,\n",
       " 1.0178728989648267,\n",
       " 0.8254736369771758,\n",
       " 0.7240364214704467,\n",
       " 0.7042620088852867,\n",
       " 0.8045166586998127,\n",
       " 1.0654464323914246,\n",
       " 0.6756309431009027,\n",
       " 1.0844195407888644,\n",
       " 0.6584795240831962,\n",
       " 0.5424545589623073,\n",
       " 0.2754554444560433,\n",
       " 0.6759724673761993,\n",
       " 0.5323111915714543,\n",
       " 0.4710324905112673,\n",
       " 0.5974818409325071,\n",
       " 0.866926409478066,\n",
       " 0.7867578345683397,\n",
       " 0.9741084415270349,\n",
       " 0.9202274366969541,\n",
       " 0.8238250116840289,\n",
       " 0.5017912913529643,\n",
       " 0.500711465333609,\n",
       " 0.5982558391742387,\n",
       " 0.5608030773929563,\n",
       " 0.5874565286597806,\n",
       " 1.0538483815117656,\n",
       " 0.7268889499883481,\n",
       " 0.5391050287832961,\n",
       " 0.811717100576165,\n",
       " 0.8033532130557156,\n",
       " 0.3826137794481961,\n",
       " 0.5206268353083864,\n",
       " 0.14940033548417417,\n",
       " 0.9298887583081396,\n",
       " 0.6641258228691067,\n",
       " 0.5998708543591477,\n",
       " 0.4939635885570176,\n",
       " 0.7662617339832617,\n",
       " 0.6304662720192022,\n",
       " 0.4523890764487165,\n",
       " 0.46899143660138953,\n",
       " 0.9737743366957821,\n",
       " 0.6174131469593653,\n",
       " 0.5628067789665063,\n",
       " 1.0234112811639644,\n",
       " 0.25909946526467864,\n",
       " 0.8497690871947212,\n",
       " 0.9056919925466076,\n",
       " 0.8154322003948067,\n",
       " 0.7709615880397862,\n",
       " 0.9310201839965817,\n",
       " 0.8863547126843366,\n",
       " 0.08794822328814014,\n",
       " 0.6093303257045513,\n",
       " 0.8461490342993517,\n",
       " 0.6118170191930654,\n",
       " ...]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5042884234117748"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans = []\n",
    "for item in final:\n",
    "    if item >= 0.75:\n",
    "        ans.append(1)\n",
    "    else:\n",
    "        ans.append(0)\n",
    "sum(ans) / len(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46399</th>\n",
       "      <td>46399</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46400</th>\n",
       "      <td>46400</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46401</th>\n",
       "      <td>46401</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46402</th>\n",
       "      <td>46402</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46403</th>\n",
       "      <td>46403</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>46404 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  target\n",
       "0          0       0\n",
       "1          1       0\n",
       "2          2       0\n",
       "3          3       0\n",
       "4          4       0\n",
       "...      ...     ...\n",
       "46399  46399       1\n",
       "46400  46400       0\n",
       "46401  46401       1\n",
       "46402  46402       1\n",
       "46403  46403       1\n",
       "\n",
       "[46404 rows x 2 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission = pd.read_csv('/home/mglee/VSCODE/Dacon/Jobcare_data/sample_submission.csv')\n",
    "sample_submission['target'] = ans\n",
    "sample_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.to_csv(\"/home/mglee/VSCODE/Dacon/Jobcare_data/prediction_0118.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c6f34507fa43ba317958b721fa8398d2051b96ef3f3b32ff98429c26ce06f8cf"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('svmglee': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
